Boa ‚Äî √© um tema importante. Achei informa√ß√µes documentadas e relatos da comunidade sobre falhas / problemas de mem√≥ria com RabbitMQ executando em cont√™iner / Kubernetes. Com base a√≠, e considerando o que d√° para controlar via ‚Äúprobes‚Äù + configura√ß√£o + boas pr√°ticas, abaixo vai um diagn√≥stico + recomenda√ß√µes de probes / alertas / configura√ß√£o para **evitar ou mitigar falhas de mem√≥ria**.

---

## üîé Problemas/Contextos de falhas de mem√≥ria no RabbitMQ

* O RabbitMQ usa um mecanismo interno de limite de mem√≥ria (`vm_memory_high_watermark`) para bloquear publishers quando o uso de mem√≥ria ultrapassa um limiar. ([rabbitmq.com][1])
* Em ambientes containerizados / Kubernetes, esse limiar √© particularmente relevante ‚Äî e recomenda-se usar **limite absoluto** em vez de relativo, pois a detec√ß√£o autom√°tica de mem√≥ria dispon√≠vel pode estar errada. ([rabbitmq.com][1])
* H√° relatos de inst√¢ncias onde, mesmo com watermark configurado, o uso de mem√≥ria disparou ‚Äî por exemplo: ‚Äúuso de 0.5 GB ‚Üí 9 GB em 10 minutos‚Äù sem aumento percept√≠vel nas m√©tricas de mensagens, canais, conex√µes etc. ([Google Groups][2])
* Em Kubernetes, com `limits/requests` insuficientes, o cont√™iner pode ser OOMKilled mesmo antes de o RabbitMQ ‚Äúperceber‚Äù que est√° usando muita mem√≥ria. ([GitHub][3])
* Um caso recente (2025) relata que ap√≥s upgrade para vers√£o 3.13.6, o broker ‚Äúcrasha‚Äù sob certas cargas ‚Äî por exemplo, ao buscar 10 000 mensagens via HTTP API com ack autom√°tico, especialmente quando filas grandes (~100 MB) ‚Äî mesmo sem pico vis√≠vel de RAM externamente. ([GitHub][4])

Conclus√£o: mesmo com configura√ß√µes ‚Äúpadr√£o‚Äù, h√° cen√°rios reais de sobrecarga de mem√≥ria / falha de mem√≥ria / OOM + alarmes de mem√≥ria no RabbitMQ em Kubernetes. Por isso √© prudente adotar probes e alertas pr√≥-ativos ‚Äî n√£o apenas confiar no watermark interno.

---

## ‚úÖ Estrat√©gias e configura√ß√µes recomendadas para mitigar riscos de mem√≥ria

Antes das probes, vale ajustar o RabbitMQ para reduzir chances de uso excessivo:

* **Usar `vm_memory_high_watermark.absolute`** ‚Äî definir explicitamente um valor fixo compat√≠vel com os recursos alocados ao pod. Ex: `vm_memory_high_watermark.absolute = 4Gi`. ([rabbitmq.com][1])
* **Garantir requests/limits adequados no pod (RAM & CPU)**; evitar usar valores baixos sob risco de OOM. ([GitHub][3])
* **Limitar backlog de mensagens, filas e conex√µes** ‚Äî muitos dados apontam que ac√∫mulo de mensagens n√£o consumidas, muitas filas, muitas conex√µes/canais lentos ou clientes abrindo/fechando conex√µes frequentemente s√£o causas comuns de alta mem√≥ria / CPU. ([CloudAMQP][5])
* **Preferir filas que minimizam uso de RAM**, se poss√≠vel ‚Äî ex: usar filas ‚Äúlazy‚Äù ou vers√µes de fila que persistem no disco, quando o uso de mem√≥ria deve ser reduzido. ([CloudAMQP][5])

Mas isso n√£o elimina totalmente o risco ‚Äî por isso, vamos √† parte pr√°tica: *probes e alertas*.

---

## üõ† Probes / Liveness- & Readiness-Probes + alertas para Kubernetes + Observabilidade

Aqui v√£o sugest√µes de como configurar probes + monitoramento para identificar antes que o n√≥ sofra falhas graves de mem√≥ria / OOM / bloqueio.

### üîπ Liveness / Readiness Probes (Kubernetes)

Voc√™ pode criar probes customizados (HTTP / exec / script) que verificam:

* **Uso de mem√≥ria do processo RabbitMQ** (RSS, ou mem√≥ria relatada pela API do RabbitMQ). Se ultrapassar um limiar ‚Äúseguro‚Äù (ex: 75‚Äì85% do memory limit do pod), marcar como falha ‚Üí pod ser√° reiniciado.
* **Status de ‚Äúmemory alarm‚Äù do RabbitMQ** ‚Äî atrav√©s da API HTTP de gerenciamento ou CLI (`rabbitmq-diagnostics status` + filtro), verificando se h√° alarmes de mem√≥ria/disco. Se alarmes ativos h√° mais que X tempo, marcar como ‚Äúunready‚Äù ou ‚Äúliveness fail‚Äù.
* **N√∫mero de mensagens nas filas + backlog n√£o consumido** ‚Äî se existirem filas muito grandes ou backlog crescente por muito tempo, sinalizar problema.

Exemplo de probe (pseudoc√≥digo shell + Kubernetes YAML):

```yaml
livenessProbe:
  exec:
    command:
      - /bin/sh
      - -c
      - |
        # usa rabbitmq-diagnostics ou API HTTP para checar
        mem=$(rabbitmq-diagnostics status -q | grep memory | awk '{print $2}')
        limit=4Gi  # ou parse de env
        perc=$(expr $mem \* 100 / $limit)
        [ $perc -lt 85 ]
  initialDelaySeconds: 60
  periodSeconds: 30
```

Ou via HTTP:

```yaml
readinessProbe:
  httpGet:
    path: /api/healthchecks/node
    port: 15672
    scheme: HTTP
  initialDelaySeconds: 30
  periodSeconds: 20
```

(com endpoint de healthcheck customizado que inclua verifica√ß√£o de memory alarm e uso de mem√≥ria)

### üîπ Monitoramento + Alertas com Prometheus / Grafana (observabilidade)

* Expor m√©tricas de mem√≥ria via `rabbitmq_exporter` ou via API de status. Monitorar: mem√≥ria alocada, mem√≥ria usada, ‚Äúallocated_unused‚Äù, pools Erlang, uso de page cache, n√∫mero de filas, n√∫mero de consumidores, backlog por fila. ([rabbitmq.com][6])
* Criar alertas como:

  * Se mem√≥ria usada > 70‚Äì80% do limite por mais de N minutos.
  * Se ‚Äúmemory alarm‚Äù ativado /  node em ‚Äúalarm state‚Äù.
  * Se n√∫mero de mensagens n√£o consumidas em filas > threshold (ex: backlog > X mensagens ou > Y MB por fila).
  * Se n√∫mero de filas ou conex√µes / canais cresce abruptamente.
  * Backlog crescendo por mais de N minutos (sinal de consumidor lento ou vazamento).

Esses alertas permitem a√ß√£o preventiva: escala de consumidores, purga de filas antigas, rein√≠cio controlado, investiga√ß√£o de leaks.

### üîπ Estrat√©gia de rein√≠cio controlado / ‚Äúcircuit breaker‚Äù

Se os alertas acima dispararem (ex: mem√≥ria alta cont√≠nua, alarmes, backlog persistente), seu sistema pode:

* Migrar consumidores para outro broker / node (se cluster).
* Reiniciar o pod com tempo de ‚Äúdrain‚Äù ‚Äî desativar publishers, esperar consumir mensagens, depois reiniciar com seguran√ßa.
* Notificar time SRE / Dev para investiga√ß√£o (logs, poss√≠veis leaks, problemas de throughput).

---

## üîÑ Exemplo de YAML com probes + ajustes de configura√ß√£o (para Kubernetes)

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: rabbitmq
        image: rabbitmq:3.13.6
        resources:
          requests:
            memory: "4Gi"
            cpu: "1"
          limits:
            memory: "6Gi"
            cpu: "2"
        env:
        - name: RABBITMQ_VM_MEMORY_HIGH_WATERMARK_ABSOLUTE
          value: "4Gi"
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              mem=$(rabbitmq-diagnostics status -q | grep memory | awk '{print $2}' | sed 's/[^0-9]//g')
              limit=4096
              if [ $mem -lt $(($limit * 85 / 100)) ]; then exit 0; else exit 1; fi
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /api/healthchecks/node
            port: 15672
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 20
```

> Obs: o endpoint `/api/healthchecks/node` pode ser substitu√≠do por script custom que verifica alarmes de mem√≥ria/disco + backlog.

---

## ‚ö†Ô∏è Limita√ß√µes e cuidados

* O watermark interno **n√£o previne totalmente** uso excessivo de mem√≥ria ‚Äî apenas bloqueia publishers; se as filas acumularem mensagens (muitas ou grandes), a mem√≥ria ainda pode crescer at√© OOM. ([rabbitmq.com][1])
* Em cgroup / Kubernetes, o RabbitMQ pode ‚Äúver‚Äù menos mem√≥ria que o dispon√≠vel, especialmente se n√£o estiver informando limites corretamente ‚Äî da√≠ a recomenda√ß√£o de usar `absolute`. ([Google Groups][7])
* Probes ‚Äúexec‚Äù que chamam `rabbitmq-diagnostics` ou acessam a API consomem recursos; calibra√ß√£o de frequ√™ncia/pol√≠tica √© importante para n√£o sobrecarregar o broker.
* Reiniciar bruscamente pode causar perda de mensagens em mem√≥ria ou delays ‚Äî preferir shutdown ‚Äúgracioso‚Äù, drain de filas, se poss√≠vel.

---

Se quiser ‚Äî posso montar **um conjunto completo de alertas Prometheus + regras de Grafana + YAML de probes readiness/liveness + config recomendada** para usar com RabbitMQ em Kubernetes, j√° adaptado a produ√ß√£o ‚Äî isso ajuda bastante a evitar incidentes. Quer que monte pra voc√™ agora?

[1]: https://www.rabbitmq.com/docs/memory?utm_source=chatgpt.com "Memory Threshold and Limit"
[2]: https://groups.google.com/g/rabbitmq-users/c/GmX2Ax5sZKg?utm_source=chatgpt.com "RabbitMQ server is out of memory without clear cause"
[3]: https://github.com/rabbitmq/cluster-operator/discussions/1099?utm_source=chatgpt.com "Question about RabbitMQ Memory Usage especially ..."
[4]: https://github.com/rabbitmq/rabbitmq-server/discussions/13060?utm_source=chatgpt.com "3.13.6 Node memory spike when polling for 10K messages ..."
[5]: https://www.cloudamqp.com/blog/identify-and-protect-against-high-cpu-and-memory-usage.html?utm_source=chatgpt.com "How to handle and avoid high CPU or Memory usage"
[6]: https://www.rabbitmq.com/docs/memory-use?utm_source=chatgpt.com "Reasoning About Memory Use"
[7]: https://groups.google.com/g/rabbitmq-users/c/ESnCf64vfZ0?utm_source=chatgpt.com "Running RabbitMQ inside container - cgroup (docker) ..."
